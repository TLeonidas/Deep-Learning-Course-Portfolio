{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1. Introduction\n",
        "* **Text as a Challenge**: Unlike numerical data, raw text is unstructured and messy. This makes it hard for computers to directly analyze and uncover insights.\n",
        "* **Vectorization to the Rescue**: Vectorization techniques transform words, sentences, and even entire documents into numerical representations. This allows us to use mathematical and computational tools for powerful text analysis.\n",
        "* **Your Mission**: This assignment will take you on a journey through text processing and vectorization. You'll decode clues, uncover hidden connections, and collaborate with others to reach the ultimate treasure!"
      ],
      "metadata": {
        "id": "t3gJ6nOQD3hP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Setting Up\n",
        "* Install the necessary libraries\n",
        "* Import the  libraries\n",
        "* Load the Dataset\n",
        "\n",
        "##Make sure you have these libraries installed##\n",
        " (pip install [library_name] if needed):\n",
        "* nltk\n",
        "* pandas\n",
        "* sklearn\n",
        "* gensim\n",
        "* spacy\n",
        "* (Optional for advanced exploration): transformers\n",
        "\n"
      ],
      "metadata": {
        "id": "NTdDcAapFSHf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McRTSlMnBCh_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d945b3f4-1bcc-4213-ae30-040e4cafac78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Install libraries if needed (uncomment as necessary)\n",
        "# !pip install nltk pandas sklearn gensim spacy transformers\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re  # For regular expressions\n",
        "import gensim\n",
        "import spacy\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Optional advanced exploration with Transformers\n",
        "from transformers import pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.  Your Quest Begins – The Initial Clue\n",
        "* Decipher the Message: Your first clue is the key! Analyze it closely. What words or themes stand out?\n",
        "* * Hint 1: Think about which topic category within the Newsgroup 20 dataset connects to your initial clue."
      ],
      "metadata": {
        "id": "RvGnUmbOGVxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Load the 20 newsgroups dataset\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "\n",
        "# Define the text to classify\n",
        "text = \"I shield secrets with mathematical might, transforming messages into hidden light\"\n",
        "\n",
        "# Create a pipeline that combines TF-IDF vectorization and Multinomial Naive Bayes classifier\n",
        "pipe = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "\n",
        "# Train the model on the training data\n",
        "pipe.fit(newsgroups_train.data, newsgroups_train.target)\n",
        "\n",
        "# Predict the category of the text\n",
        "predicted_category = pipe.predict([text])[0]\n",
        "\n",
        "# Get the target names (category names)\n",
        "target_names = newsgroups_train.target_names\n",
        "\n",
        "# Print the predicted category\n",
        "print(\"Predicted Category:\", target_names[predicted_category])\n"
      ],
      "metadata": {
        "id": "wFQiI91mGytN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e86916c-2833-48b7-c70f-b42868d330ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Category: sci.crypt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Keyword Quest\n",
        "Finding the Guiding Stars: Time to extract keywords that illuminate your path. Let's start with TF-IDF:\n"
      ],
      "metadata": {
        "id": "uCxTucp7GzWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts.\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Normalize, remove possessives, lemmatize, and remove stopwords.\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"'s\\b\", \"\", text)\n",
        "    text = re.sub(r\"[^\\w\\s]\", '', text)\n",
        "    words = word_tokenize(text)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words if w not in stop_words]\n",
        "    return lemmatized_words\n",
        "\n",
        "def extract_keywords(text_data):\n",
        "    \"\"\"Extract keywords using TF-IDF from preprocessed text.\"\"\"\n",
        "    stop_words = stopwords.words('english')\n",
        "    vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
        "    text_data = [' '.join(clean_text(doc)) for doc in text_data]\n",
        "    vectors = vectorizer.fit_transform(text_data)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    dense = vectors.todense()\n",
        "    denselist = dense.tolist()\n",
        "    df = pd.DataFrame(denselist, columns=feature_names)\n",
        "    return df\n",
        "\n",
        "# Clues\n",
        "my_text = [\n",
        "    \"From ancient ciphers to digital keys, I weave patterns of security, where only the intended eyes can see.\",\n",
        "    \"Like a dance with logic, steps in a line, my pattern dictates the final design.\",\n",
        "    \"Logic's blueprint, code's design, my instructions make machines align.\"\n",
        "]\n",
        "\n",
        "keyword_df = extract_keywords(my_text)\n",
        "print(keyword_df)"
      ],
      "metadata": {
        "id": "towQxMlhG92o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47bd3dc3-67c1-40dc-8605-915c881e88d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      align   ancient  blueprint    cipher      code     dance    design  \\\n",
            "0  0.000000  0.323112   0.000000  0.323112  0.000000  0.000000  0.000000   \n",
            "1  0.000000  0.000000   0.000000  0.000000  0.000000  0.359554  0.273450   \n",
            "2  0.373801  0.000000   0.373801  0.000000  0.373801  0.000000  0.284285   \n",
            "\n",
            "    dictate   digital       eye  ...      like      line     logic   machine  \\\n",
            "0  0.000000  0.323112  0.323112  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "1  0.359554  0.000000  0.000000  ...  0.359554  0.359554  0.273450  0.000000   \n",
            "2  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.284285  0.373801   \n",
            "\n",
            "       make   pattern  security       see      step     weave  \n",
            "0  0.000000  0.245735  0.323112  0.323112  0.000000  0.323112  \n",
            "1  0.000000  0.273450  0.000000  0.000000  0.359554  0.000000  \n",
            "2  0.373801  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "\n",
            "[3 rows x 24 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hint 2:\n",
        " Look for keywords that might link to other texts, reveal new concepts, or hint at hidden patterns within the data."
      ],
      "metadata": {
        "id": "pIt9nP5VHAP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Semantic Safari\n",
        "* Exploring the World of Meaning: Word embeddings like Word2Vec or GloVe help us understand how words relate to each other.\n",
        "Hint 3: Calculate similarities between your keywords and texts in other categories. Could there be unexpected connections?\n",
        "\n",
        "## Hint 3:\n",
        "Calculate similarities between your keywords and texts in other categories. Could there be unexpected connections?\n"
      ],
      "metadata": {
        "id": "8kFaZ8vgHJ9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "sentences = [clean_text(text) for text in my_text]\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n"
      ],
      "metadata": {
        "id": "-TRQhrPGJkkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying vocabulary\n",
        "print(\"Vocabulary in Model:\", model.wv.index_to_key)\n",
        "\n",
        "# Calculate and display similarities\n",
        "for text in my_text:\n",
        "    preprocessed_text = clean_text(text)\n",
        "    text_vector = np.mean([model.wv[word] for word in preprocessed_text if word in model.wv], axis=0)\n",
        "\n",
        "    # Dictionary to store keyword and its similarity\n",
        "    similarity_dict = {}\n",
        "    for keyword in keyword_df.columns.tolist():\n",
        "        if keyword in model.wv:\n",
        "            keyword_vector = model.wv[keyword]\n",
        "            similarity = np.dot(keyword_vector, text_vector) / (np.linalg.norm(keyword_vector) * np.linalg.norm(text_vector))\n",
        "            similarity_dict[keyword] = similarity\n",
        "        else:\n",
        "            similarity_dict[keyword] = float('-inf')  # Treat words not in vocab as lowest similarity\n",
        "\n",
        "    # Sorting the dictionary by similarity in descending order\n",
        "    sorted_similarities = sorted(similarity_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "    # Printing the sorted similarities\n",
        "    print(f\"\\nComparing to text: {' '.join(preprocessed_text)}\")\n",
        "    for keyword, sim_value in sorted_similarities:\n",
        "        if sim_value == float('-inf'):\n",
        "            print(f\"Keyword '{keyword}' not in vocabulary.\")\n",
        "        else:\n",
        "            print(f\"Similarity with '{keyword}': {sim_value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M55JT3c1J1HC",
        "outputId": "760ccd03-4d04-47a0-856a-7170a077eb6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary in Model: ['design', 'pattern', 'logic', 'align', 'like', 'cipher', 'digital', 'key', 'weave', 'security', 'intend', 'eye', 'see', 'dance', 'machine', 'step', 'line', 'dictate', 'final', 'blueprint', 'code', 'instruction', 'make', 'ancient']\n",
            "\n",
            "Comparing to text: ancient cipher digital key weave pattern security intend eye see\n",
            "Similarity with 'intend': 0.4606\n",
            "Similarity with 'ancient': 0.3766\n",
            "Similarity with 'eye': 0.3578\n",
            "Similarity with 'cipher': 0.3250\n",
            "Similarity with 'see': 0.3088\n",
            "Similarity with 'digital': 0.2754\n",
            "Similarity with 'key': 0.2660\n",
            "Similarity with 'weave': 0.2631\n",
            "Similarity with 'pattern': 0.2382\n",
            "Similarity with 'security': 0.2108\n",
            "Similarity with 'final': 0.1184\n",
            "Similarity with 'make': 0.1177\n",
            "Similarity with 'design': 0.1173\n",
            "Similarity with 'blueprint': 0.0964\n",
            "Similarity with 'logic': 0.0755\n",
            "Similarity with 'machine': 0.0659\n",
            "Similarity with 'align': 0.0626\n",
            "Similarity with 'dictate': 0.0334\n",
            "Similarity with 'step': 0.0272\n",
            "Similarity with 'line': -0.0061\n",
            "Similarity with 'instruction': -0.0390\n",
            "Similarity with 'code': -0.0414\n",
            "Similarity with 'dance': -0.0423\n",
            "Similarity with 'like': -0.0819\n",
            "\n",
            "Comparing to text: like dance logic step line pattern dictate final design\n",
            "Similarity with 'final': 0.4735\n",
            "Similarity with 'like': 0.4502\n",
            "Similarity with 'pattern': 0.4455\n",
            "Similarity with 'line': 0.3894\n",
            "Similarity with 'dance': 0.3858\n",
            "Similarity with 'logic': 0.3096\n",
            "Similarity with 'step': 0.3056\n",
            "Similarity with 'design': 0.2933\n",
            "Similarity with 'dictate': 0.2932\n",
            "Similarity with 'key': 0.1158\n",
            "Similarity with 'code': 0.0927\n",
            "Similarity with 'cipher': 0.0442\n",
            "Similarity with 'eye': 0.0405\n",
            "Similarity with 'make': 0.0375\n",
            "Similarity with 'machine': 0.0234\n",
            "Similarity with 'digital': 0.0215\n",
            "Similarity with 'see': 0.0127\n",
            "Similarity with 'ancient': -0.0074\n",
            "Similarity with 'weave': -0.0154\n",
            "Similarity with 'instruction': -0.0846\n",
            "Similarity with 'security': -0.0957\n",
            "Similarity with 'intend': -0.1294\n",
            "Similarity with 'align': -0.1567\n",
            "Similarity with 'blueprint': -0.1781\n",
            "\n",
            "Comparing to text: logic blueprint code design instruction make machine align\n",
            "Similarity with 'blueprint': 0.4402\n",
            "Similarity with 'align': 0.3932\n",
            "Similarity with 'machine': 0.3857\n",
            "Similarity with 'code': 0.3833\n",
            "Similarity with 'instruction': 0.3478\n",
            "Similarity with 'make': 0.3472\n",
            "Similarity with 'design': 0.3134\n",
            "Similarity with 'logic': 0.2154\n",
            "Similarity with 'eye': 0.1748\n",
            "Similarity with 'see': 0.1203\n",
            "Similarity with 'key': 0.1177\n",
            "Similarity with 'digital': 0.1062\n",
            "Similarity with 'weave': 0.0717\n",
            "Similarity with 'pattern': 0.0528\n",
            "Similarity with 'intend': 0.0343\n",
            "Similarity with 'dictate': 0.0077\n",
            "Similarity with 'final': -0.0004\n",
            "Similarity with 'line': -0.0127\n",
            "Similarity with 'ancient': -0.0369\n",
            "Similarity with 'step': -0.0426\n",
            "Similarity with 'security': -0.0464\n",
            "Similarity with 'dance': -0.0475\n",
            "Similarity with 'cipher': -0.0624\n",
            "Similarity with 'like': -0.0694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Exploration: Transformers (Optional)\n",
        "While Word2Vec and GloVe offer valuable insights, Transformer-based models can provide even more nuanced semantic understanding. These models go beyond individual word meanings and capture context-dependent relationships between words.\n",
        "\n",
        "* **Exploring with Transformers**: Consider using pre-trained Transformers for tasks like question answering or text summarization.\n",
        "* * Imagine you have a question related to the content you've analyzed. You could use a question-answering pipeline to find the answer within relevant texts.\n",
        "* * Text summarization pipelines could be helpful for generating concise summaries of lengthy documents you encounter during your exploration.\n",
        "* Explore the Transformers documentation (https://huggingface.co/docs/transformers/en/index) to discover more pipelines and fine-tune their exploration.\n",
        "\n",
        "**Benefits and Considerations**:\n",
        "* Transformers can potentially uncover deeper semantic relationships compared to traditional word embeddings.\n",
        "* They often require more computational resources\n",
        "\n",
        "* The Transformers section below provides a commented-out example using a question-answering pipeline. You can experiment with other functionalities offered by Transformers based on their interests."
      ],
      "metadata": {
        "id": "Zo_uW51_KV0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9JOzOz6HnlD",
        "outputId": "1e31ea25-9269-490e-adea-8f32c39ec955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore Transformers (optional challenge)\n",
        "# This section requires installing the transformers library\n",
        "\n",
        "# After installing transformers (see above), uncomment the following:\n",
        "\n",
        "# Example using a pre-trained model for question answering\n",
        "answerer = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
        "\n",
        "# Example using a pre-trained model for text classification\n",
        "#classifier = pipeline(\"text-classification\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "#result = classifier(\"Like a dance with logic, steps in a line, my pattern dictates the final design.\")\n",
        "#print(result)\n",
        "\n",
        "your_text_data = \"Logic's blueprint, code's design, my instructions make machines align.\"\n",
        "\n",
        "# Imagine your question relates to the content of your texts\n",
        "question = \"Is the text referring to what kind of design?\"\n",
        "answer = answerer({'question': question, 'context': your_text_data}) #Replace_text_data with the actual text data you will use for you exploration.\n",
        "print(question)\n",
        "print(answer)\n",
        "\n",
        "# Experiment with other Transformers pipelines like text classification,\n",
        "# summarization, or sentiment analysis based on your exploration goals.\n"
      ],
      "metadata": {
        "id": "6xI2ScjULjJB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cea3d81-0eae-40c8-f635-04736e2df41c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is the text referring to what kind of design?\n",
            "{'score': 0.41227254271507263, 'start': 19, 'end': 32, 'answer': \"code's design\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Pattern Pursuit\n",
        "* **Cracking the Code**: Examine closely for unusual patterns within the texts – letter sequences, numbers, or anything resembling a code. Regular expressions will be your powerful ally.\n",
        "##Hint 4:\n",
        " Need help learning regular expressions? Check out this resource: https://docs.python.org/3/library/re.html"
      ],
      "metadata": {
        "id": "mz3wjdR7Hvrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regular expression examples\n",
        "'''\n",
        "text = \"There is a hidden code: AB12XY94\"\n",
        "pattern = r\"\\b[A-Z]{2}\\d{2}[A-Z]{2}\\d{2}\\b\"  # Pattern for a simple code-like structure\n",
        "matches = re.findall(pattern, text)\n",
        "if matches:\n",
        "    print(\"Found potential codes:\", matches)\n",
        "\n",
        "    import re\n",
        "'''\n",
        "def find_patterns(text_data):\n",
        "    # Updated pattern for typical US license plates (e.g., \"XYZ1234\")\n",
        "    license_plate_pattern = r\"\\b[A-Z]{3}\\d{4}\\b\"\n",
        "    # Standard email regex pattern\n",
        "    email_pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\"\n",
        "    # Pattern for typical US phone numbers (e.g., \"555-333-8888\")\n",
        "    phone_pattern = r\"\\b\\d{3}-\\d{3}-\\d{4}\\b\"\n",
        "\n",
        "    license_plates = re.findall(license_plate_pattern, text_data)\n",
        "    emails = re.findall(email_pattern, text_data)\n",
        "    phone_numbers = re.findall(phone_pattern, text_data)\n",
        "\n",
        "    # Reporting findings\n",
        "    if license_plates:\n",
        "        print(\"Found potential license plates:\", license_plates)\n",
        "    if emails:\n",
        "        print(\"Found potential emails:\", emails)\n",
        "    if phone_numbers:\n",
        "        print(\"Found phone numbers:\", phone_numbers)\n",
        "    if not (license_plates or emails or phone_numbers):\n",
        "        print(\"No potential patterns found.\")\n",
        "\n",
        "# Example usage (students would apply this to their texts)\n",
        "my_text = \"The license plate XYZ1234 is expired, you can renew it contacting texasdps@gov.us or call 555-333-8888.\"\n",
        "find_patterns(my_text)"
      ],
      "metadata": {
        "id": "4smDrRZIN0l8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0edbca23-a87b-419a-dfce-9e5945bf083e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found potential license plates: ['XYZ1234']\n",
            "Found potential emails: ['texasdps@gov.us']\n",
            "Found phone numbers: ['555-333-8888']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clue 1a = sci.crypt\n",
        "## Clue 2a = security\n",
        "## Clue 1b = logic\n",
        "## Clue 2b = code's design"
      ],
      "metadata": {
        "id": "5gNZh79ziU6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Collaboration and Convergence\n",
        "* **Teamwork Makes the Dream Work** How will your team share your findings and combine your insights? Discuss effective communication strategies.\n",
        "* **The Final Puzzle**: Once all the clues are gathered, collaborate to solve the ultimate puzzle and locate the treasure!"
      ],
      "metadata": {
        "id": "auFO6H2zIEmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Reflection and Report\n",
        "* Document Your Journey: Your final report is crucial! It should include:\n",
        "* * The methods and techniques you used at each stage.\n",
        "* * Explain in details what the Code snippets provided do and why.\n",
        "* * Insights on your collaboration process.\n",
        "* Lessons Learned: Think about:\n",
        "* * Which text processing techniques were most helpful and why?\n",
        "* * How did vectorization empower you to find hidden connections?\n",
        "* * What was the most surprising part of this adventure?\n",
        "* * How could you use these skills for other problems in the real world?"
      ],
      "metadata": {
        "id": "9pRGYbrFIdKC"
      }
    }
  ]
}